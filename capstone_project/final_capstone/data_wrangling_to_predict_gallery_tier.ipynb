{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration to Predict Gallery Tier\n",
    "### First step towards building an objective Gallery Qualification Score\n",
    "**Author: Nicholas Sewitz  \n",
    "Contributors: Anil Bawa-Cavia, Will Goldstein**\n",
    "\n",
    "This notebook processes certain predetermined features that we believe are predictive of the qualitative gallery tier score applied by the GFI team. The goal is to identify what features and models should be used to build a more ambitious objective qualification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import datetime\n",
    "sys.path.append(os.environ['minotaur'])\n",
    "\n",
    "import os.path\n",
    "import http\n",
    "import urllib.request \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from socket import timeout\n",
    "import boto3\n",
    "import gzip\n",
    "import io\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14,3\n",
    "\n",
    "from dbs import redshift\n",
    "redshift.connect()\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "from pygeocoder import Geocoder\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "from slugify import slugify\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDRegressor, SGDClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, accuracy_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locations from Bearden (In Progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_locations = redshift.execute(\"\"\"\n",
    "SELECT website AS \"domain\",\n",
    "       latitude,\n",
    "       longitude\n",
    "FROM bearden_exports.organizations\n",
    "WHERE latitude IS NOT NULL and latitude != 0\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artsy Partners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partners = redshift.execute(\"\"\"\n",
    "SELECT website as \"domain\"\n",
    "FROM gravity.partners\n",
    "where website is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artsy Partners with Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "partner_artists = redshift.execute(\"\"\"\n",
    "SELECT \n",
    "       p.website AS \"domain\",\n",
    "       a.slug AS artist_slug\n",
    "FROM gravity.partners p\n",
    "  LEFT JOIN gravity.partner_artists pa ON p.id = pa.partner_id\n",
    "  LEFT JOIN gravity.artists a ON pa.artist_id = a.id\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artist Bids and Inquiries on Artsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artsy_artists = redshift.execute(\"\"\"\n",
    "SELECT artists.slug AS artist_slug,\n",
    "       COUNT(DISTINCT inquiry_requests.inquiry_id ) AS \"inquiry_requests_count\",\n",
    "       SUM(sale_artworks.artsy_bid_count) AS \"bid_count\"\n",
    "FROM gravity.artists\n",
    "  LEFT JOIN analytics.artworks ON artworks.artist_id = artists.id\n",
    "  LEFT JOIN analytics.inquiry_requests AS inquiry_requests\n",
    "         ON artworks.id = inquiry_requests.inquireable_id\n",
    "        AND inquiry_requests.inquireable_type = 'Artwork'\n",
    "  LEFT JOIN analytics.sale_artworks ON sale_artworks.artwork_id = artworks.id\n",
    "GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gallery Tier (From Salesforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salesforce_partners = redshift.execute(\"\"\"\n",
    "SELECT website as domain,\n",
    "       gallery_tier\n",
    "FROM analytics.salesforce_companies\n",
    "WHERE website is not null and website != '' and gallery_tier is not null and gallery_tier != '' and \"type\" IN ('Gallery','Design Gallery')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Longer in Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "closed = redshift.execute(\"\"\"\n",
    "SELECT sc.website as domain,disqualified_reason\n",
    "FROM analytics.salesforce_companies sc\n",
    "WHERE sc.\"type\" IN ('Gallery', 'Design Gallery')\n",
    "AND sc.website IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redshift.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bearden Artists Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists = pd.read_csv('artist_rosters.csv',index_col='domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del artists['organization_type']\n",
    "del artists['notes']\n",
    "del artists['specialization']\n",
    "del artists['updated_website']\n",
    "del artists['roster_url']\n",
    "del artists['roster_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artsy Artist Page Search Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search = pd.read_csv('artist_search_analytics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del search['CTR']\n",
    "del search['Position']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artist Keyword Search Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_volume = pd.read_csv('artist-organic-search-volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Begin Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_artists = artists.iloc[:,2:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_artists = pd.DataFrame(bearden_artists.unstack().dropna()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_artists['artist_slug'] = bearden_artists[0].apply(lambda x: slugify(x))\n",
    "bearden_artists['source'] = 'bearden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search_volume['artist_slug'] = search_volume['Keyword'].apply(lambda x: str(x))\n",
    "search_volume['artist_slug'] = search_volume['artist_slug'].apply(lambda x: slugify(x))\n",
    "del search_volume['Keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_domain(website):\n",
    "    website = website.replace(\"http://\",\"\")\n",
    "    website = website.replace(\"http:\",\"\")    \n",
    "    website = website.replace(\"https://\",\"\") \n",
    "    website = website.replace(\"http:\",\"\")    \n",
    "    website = re.sub(r'(www.)(?!com)',r'',website)\n",
    "    urls = website.split('/')[0]\n",
    "    urls = urls.replace(\"]\",\"\")\n",
    "    urls = urls.replace(\"[\",\"\")\n",
    "    urls = urls.strip('.')\n",
    "    return urls\n",
    "bearden_artists.domain = bearden_artists.domain.apply(strip_domain)\n",
    "salesforce_partners.domain = salesforce_partners.domain.apply(strip_domain)\n",
    "partners.domain = partners.domain.apply(strip_domain)\n",
    "closed.domain = closed.domain.apply(strip_domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_artists = bearden_artists[['domain','artist_slug','source']].copy()\n",
    "bearden_artists = bearden_artists.drop_duplicates(subset=['domain','artist_slug'], keep='last')\n",
    "closed = closed.drop_duplicates('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partner_artists.domain = partner_artists.domain.astype(str)\n",
    "partner_artists.domain = partner_artists.domain.apply(strip_domain)\n",
    "partner_artists['source'] = 'partners'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190779, 3)\n",
      "(185016, 3)\n"
     ]
    }
   ],
   "source": [
    "print(partner_artists.shape)\n",
    "print(bearden_artists.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_artists = pd.concat([bearden_artists, partner_artists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_artists = all_artists.drop_duplicates(subset=['domain','artist_slug'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artsy_artists = artsy_artists.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_artists['on_artsy'] = all_artists.artist_slug.isin(artsy_artists.artist_slug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "search['artist_slug'] = search.Pages.apply(lambda x: re.split('\\\\/artist/\\\\b',x)[-1])\n",
    "del search['Pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists_with_search = all_artists.merge(search, how=\"left\", left_on='artist_slug', right_on='artist_slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10328, 7)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 10k artists with search volume that match with the partner artists dataset\n",
    "\n",
    "test = artists_with_search.merge(search_volume, how=\"inner\", left_on='artist_slug', right_on='artist_slug')\n",
    "test.drop_duplicates('artist_slug').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists_with_search = artists_with_search.merge(search_volume, how=\"left\", left_on='artist_slug', right_on='artist_slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artists_with_search_bids_inquiries = artists_with_search.merge(artsy_artists, how=\"left\", left_on='artist_slug', right_on='artist_slug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# artists_with_search_bids_inquiries.domain = artists_with_search_bids_inquiries.domain.str.strip('.') \n",
    "# salesforce_partners.domain = salesforce_partners.domain.str.strip('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domains_with_metrics = artists_with_search_bids_inquiries[['domain','Clicks','Impressions','inquiry_requests_count','bid_count','on_artsy','search_volume']].groupby('domain').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slug_count = pd.DataFrame(artists_with_search_bids_inquiries[['domain','artist_slug']].groupby('domain').count())\n",
    "domains_with_metrics = domains_with_metrics.merge(slug_count, how=\"left\", left_index=True, right_index=True)\n",
    "salesforce_partners = salesforce_partners.drop_duplicates('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domains_with_metrics_and_demographic = salesforce_partners.merge(domains_with_metrics, how=\"left\", left_on=\"domain\",right_index=True)\n",
    "domains_with_metrics_and_demographic = domains_with_metrics_and_demographic.fillna(np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimum Distance to Art City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bearden_locations.domain = bearden_locations.domain.apply(strip_domain)\n",
    "bearden_locations = bearden_locations.drop_duplicates('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_locations = bearden_locations[['domain','latitude','longitude']].copy()\n",
    "\n",
    "domain_locations['lat_long'] = list(zip(domain_locations.latitude, domain_locations.longitude))\n",
    "del domain_locations['latitude']\n",
    "del domain_locations['longitude']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = (40.7465, 74.0014)\n",
    "london = (51.4851, 0.1749)\n",
    "los_angeles = (34.0900, 118.3617)\n",
    "paris = (48.8587, 2.3588)\n",
    "berlin = (52.5194, 13.4067)\n",
    "hong_kong = (22.3964, 114.1095)\n",
    "miami = (25.7617, 80.1918)\n",
    "venice = (45.4408, 12.3155)\n",
    "basel = (47.5596, 7.5886)\n",
    "sao_paulo = (23.5505, 46.6333)\n",
    "melbourne = (37.8136, 144.9631)\n",
    "mexico_city = (19.4326, 99.1332)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain_locations['distance_to_ny'] = domain_locations.lat_long.apply(lambda x: vincenty(x, ny).miles)\n",
    "domain_locations['distance_to_london'] = domain_locations.lat_long.apply(lambda x: vincenty(x, london).miles)\n",
    "domain_locations['distance_to_los_angeles'] = domain_locations.lat_long.apply(lambda x: vincenty(x, los_angeles).miles)\n",
    "domain_locations['distance_to_paris'] = domain_locations.lat_long.apply(lambda x: vincenty(x, paris).miles)\n",
    "domain_locations['distance_to_berlin'] = domain_locations.lat_long.apply(lambda x: vincenty(x, berlin).miles)\n",
    "domain_locations['distance_to_hong_kong'] = domain_locations.lat_long.apply(lambda x: vincenty(x, hong_kong).miles)\n",
    "domain_locations['distance_to_miami'] = domain_locations.lat_long.apply(lambda x: vincenty(x, miami).miles)\n",
    "domain_locations['distance_to_venice'] = domain_locations.lat_long.apply(lambda x: vincenty(x, venice).miles)\n",
    "domain_locations['distance_to_basel'] = domain_locations.lat_long.apply(lambda x: vincenty(x, basel).miles)\n",
    "domain_locations['distance_to_sao_paulo'] = domain_locations.lat_long.apply(lambda x: vincenty(x, sao_paulo).miles)\n",
    "domain_locations['distance_to_melbourne'] = domain_locations.lat_long.apply(lambda x: vincenty(x, melbourne).miles)\n",
    "domain_locations['distance_to_mexico_city'] = domain_locations.lat_long.apply(lambda x: vincenty(x, mexico_city).miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_columns = [col for col in domain_locations if col.startswith('distance_to_')]\n",
    "domain_locations['min_distance_to_art_city'] = domain_locations[distance_columns].min(axis=1)\n",
    "galleries_w_demographic_metric_location = domains_with_metrics_and_demographic.merge(domain_locations[['domain','min_distance_to_art_city']], how=\"left\", left_on=\"domain\",right_on=\"domain\")\n",
    "galleries_w_demographic_metric_location = galleries_w_demographic_metric_location.merge(closed,how=\"left\",left_on=\"domain\",right_on=\"domain\")\n",
    "galleries_w_demographic_metric_location['disqualified_reason'] = galleries_w_demographic_metric_location['disqualified_reason'].astype(str)\n",
    "galleries_w_demographic_metric_location['is_closed'] = [True if x.startswith('No Longer') else False for x in galleries_w_demographic_metric_location['disqualified_reason']]\n",
    "galleries_open_w_all_metrics = galleries_w_demographic_metric_location[galleries_w_demographic_metric_location['is_closed'] == False].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit DF to only include galleries with minimum # of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit dataframe to only galleries that have a tier and at least 3 features\n",
    "\n",
    "df = galleries_open_w_all_metrics.copy()\n",
    "df_with_many_features= df[(df.gallery_tier.notnull()) & (df.min_distance_to_art_city.notnull()) & (df.artist_slug > 0)]\n",
    "df_with_less_features = df[(df.gallery_tier.notnull())]\n",
    "\n",
    "\n",
    "partners['partner_on_artsy'] = True\n",
    "partners = partners.drop_duplicates('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1250 galleries have most of the features and are not on artsy, with tier\n",
    "\n",
    "df_with_less_features = df_with_less_features.merge(partners,how=\"left\",left_on=\"domain\",right_on=\"domain\").copy()\n",
    "df_with_less_features.partner_on_artsy = df_with_less_features.partner_on_artsy.fillna(False)\n",
    "\n",
    "df_with_many_features = df_with_many_features.merge(partners,how=\"left\",left_on=\"domain\",right_on=\"domain\").copy()\n",
    "df_with_many_features.partner_on_artsy = df_with_many_features.partner_on_artsy.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qualify(x):\n",
    "    if x == '1':\n",
    "        return \"very_qualified\"\n",
    "    elif x == '2':\n",
    "        return \"very_qualified\"\n",
    "    elif x == '3':\n",
    "        return \"very_qualified\"\n",
    "    elif x == '4':\n",
    "        return \"qualified\"\n",
    "    elif x == '5':\n",
    "        return \"not_qualified\"\n",
    "    \n",
    "def numeralize(x):\n",
    "    if x == '1':\n",
    "        return 100\n",
    "    elif x == '2':\n",
    "        return 100\n",
    "    elif x == '3':\n",
    "        return 100\n",
    "    elif x == '4':\n",
    "        return 50\n",
    "    elif x == '5':\n",
    "        return 0\n",
    "    \n",
    "df_with_many_features['qualified'] = df_with_many_features.gallery_tier.apply(lambda x: qualify(x))\n",
    "df_with_less_features['qualified'] = df_with_less_features.gallery_tier.apply(lambda x: qualify(x))\n",
    "\n",
    "df_with_many_features.gallery_tier = df_with_many_features.gallery_tier.apply(lambda x: numeralize(x))\n",
    "df_with_less_features.gallery_tier = df_with_less_features.gallery_tier.apply(lambda x: numeralize(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = os.environ['AWS_ID']\n",
    "aws_secret = os.environ['AWS_SECRET']\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'artsy-data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_key = 'temp/many_text.csv.gz'\n",
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "many_text = pd.read_csv(StringIO(csv_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_key = 'temp/less_text.csv.gz'\n",
    "csv_obj = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "less_text = pd.read_csv(StringIO(csv_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_many_text = df_with_many_features.merge(many_text,how='left',left_on='domain',right_on='domain')\n",
    "df_with_less_text = df_with_less_features.merge(less_text,how='left',left_on='domain',right_on='domain')\n",
    "\n",
    "del df_with_many_text['Unnamed: 0']\n",
    "del df_with_less_text['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ETag': '\"feb770e4d3765e6b5f04f5509a8ac9bc\"',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '0',\n",
       "   'date': 'Wed, 03 Jan 2018 00:27:04 GMT',\n",
       "   'etag': '\"feb770e4d3765e6b5f04f5509a8ac9bc\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': '83aQOLlVp3fyK6zUH7Cu+DfNV/LTHRUhLjJdnhELnxS3EgNELzw7YPQ6BdIlx8/WEQZJGZq9RTo=',\n",
       "   'x-amz-request-id': 'CF8D4422AC894DB4'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': '83aQOLlVp3fyK6zUH7Cu+DfNV/LTHRUhLjJdnhELnxS3EgNELzw7YPQ6BdIlx8/WEQZJGZq9RTo=',\n",
       "  'RequestId': 'CF8D4422AC894DB4',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_with_many_text.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3',aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "s3_resource.Object(bucket_name, 'temp/df_with_many_text.csv').put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ETag': '\"3d827fb72491da0201d092b400f2eec5\"',\n",
       " 'ResponseMetadata': {'HTTPHeaders': {'connection': 'close',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 03 Jan 2018 00:27:58 GMT',\n",
       "   'etag': '\"3d827fb72491da0201d092b400f2eec5\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'x-amz-id-2': 'SYTxq/c49zzxqWGewV01lLih2C+ko5Ob2vfun8yaNlVpkZYmPKmCcjvdonAmuMsKVjEy2OiBLf8=',\n",
       "   'x-amz-request-id': '61594806DFB190D4'},\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HostId': 'SYTxq/c49zzxqWGewV01lLih2C+ko5Ob2vfun8yaNlVpkZYmPKmCcjvdonAmuMsKVjEy2OiBLf8=',\n",
       "  'RequestId': '61594806DFB190D4',\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_buffer = StringIO()\n",
    "df_with_less_text.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3',aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "s3_resource.Object(bucket_name, 'temp/df_with_less_text.csv').put(Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Website Text Parse Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# url_keywords = [\"about\", \"fairs\", \"exhibitions\", \"artists\", \"home\"]\n",
    "\n",
    "# def site_text(url):\n",
    "#     try:\n",
    "#         sub_urls = []\n",
    "#         url_full = \"http://www.\"+ str(url).strip()\n",
    "#         print(url_full)\n",
    "#         page = urlopen(url_full, timeout=8)\n",
    "#         soup = BeautifulSoup(page, \"html.parser\")\n",
    "#         links = soup.find_all('a', href=True)\n",
    "#         links = [link['href'] for link in links]\n",
    "#         lower_links = [x.lower() for x in links]\n",
    "#         link_dict = dict(zip(links, lower_links))\n",
    "#         sub_urls = [k for k, v in link_dict.items() if any(xs in v for xs in url_keywords)  ]\n",
    "#         sub_urls = list(set(sub_urls))\n",
    "#         raw = soup.get_text()\n",
    "#         num = 0\n",
    "        \n",
    "#         for sub_url in sub_urls:\n",
    "#             try:\n",
    "#                 if \"http\" in sub_url:\n",
    "#                     sub_url\n",
    "#                     if sub_url.count(\"/\") > 3:\n",
    "#                         break\n",
    "#                 elif url_full.endswith(\"/\"):\n",
    "#                     sub_url = url_full + sub_url\n",
    "#                     if sub_url.count(\"/\") > 3:\n",
    "#                         break\n",
    "#                 else:\n",
    "#                     if sub_url.startswith(\"/\"):\n",
    "#                         sub_url = url_full + sub_url\n",
    "#                         if sub_url.count(\"/\") > 3:\n",
    "#                             break\n",
    "#                     else:\n",
    "#                         sub_url = url_full + \"/\" + sub_url\n",
    "#                         if sub_url.count(\"/\") > 3:\n",
    "#                             break\n",
    "#                 print(\"sub_url: \" + sub_url)\n",
    "#                 sub_page = urlopen(sub_url, timeout=8)\n",
    "#                 sub_soup = BeautifulSoup(sub_page, \"html.parser\")\n",
    "#                 raw += sub_soup.get_text()\n",
    "#                 num = num + 1\n",
    "#                 if num == 5:\n",
    "#                     break\n",
    "                    \n",
    "#             except urllib.error.URLError as e:\n",
    "#                 if hasattr(e, 'reason'): # <--\n",
    "#                     print('We failed to reach a server.')\n",
    "#                     print('Reason: ', e.reason)\n",
    "#                 elif hasattr(e, 'code'): # <--\n",
    "#                     print('The server couldn\\'t fulfill the request.')\n",
    "#                     print('Error code: ', e.code)\n",
    "#             except http.client.HTTPException as e:\n",
    "#                 if hasattr(e, 'reason'): # <--\n",
    "#                     print('We failed to reach a server.')\n",
    "#                     print('Reason: ', e.reason)\n",
    "#                 elif hasattr(e, 'code'): # <--\n",
    "#                     print('The server couldn\\'t fulfill the request.')\n",
    "#                     print('Error code: ', e.code)      \n",
    "#             except timeout:\n",
    "#                 print(\"timeout\")\n",
    "#             except KeyboardInterrupt:\n",
    "#                 raise\n",
    "#             except: \n",
    "#                 print(\"unknown\")\n",
    "                \n",
    "#         return raw\n",
    "    \n",
    "#     except urllib.error.URLError as e:\n",
    "#         if hasattr(e, 'reason'): # <--\n",
    "#             print('We failed to reach a server.')\n",
    "#             print('Reason: ', e.reason)\n",
    "#         elif hasattr(e, 'code'): # <--\n",
    "#             print('The server couldn\\'t fulfill the request.')\n",
    "#             print('Error code: ', e.code)\n",
    "#     except http.client.HTTPException as e:\n",
    "#         if hasattr(e, 'reason'): # <--\n",
    "#             print('We failed to reach a server.')\n",
    "#             print('Reason: ', e.reason)\n",
    "#         elif hasattr(e, 'code'): # <--\n",
    "#             print('The server couldn\\'t fulfill the request.')\n",
    "#             print('Error code: ', e.code)\n",
    "#     except timeout:\n",
    "#         print(\"timeout\")        \n",
    "#     except KeyboardInterrupt:\n",
    "#         raise\n",
    "#     except: \n",
    "#         print(\"unknown\")  \n",
    "\n",
    "# http://jmausolf.github.io/code/Analyzing_Text_in_Python/#analyzing_text_in_python\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
